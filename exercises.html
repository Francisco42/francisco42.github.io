<!DOCTYPE html>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Exercises - Francisco Piria</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript
      ><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
  </head>
  <body class="is-preload">
    <!-- Header -->
    <header id="header">
      <a href="index.html" class="title">Francisco Piria</a>
      <nav>
        <ul>
          <li><a href="index.html">Home</a></li>
        </ul>
      </nav>
    </header>

    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Main -->
      <section id="main" class="wrapper">
        <div class="inner">
          <h1 class="major">Exercises</h1>
          <h2 class="major"><a id="linReg">Linear regression comparison</a></h1>
          <div class="row">
            <div class="col-6 col-12-medium">
              <p>
                The objective of this exercise is to visualize and 
                compare the results of applying linear regression in two
                different ways, using the same data. All the necessary calculations
                were performed on a spreadsheet which can be found
                <a href="https://github.com/Francisco42/portafolio_IA1/tree/master/UT3%20-%20Algoritmos%20lineales">here.</a> 
                This example dataset
                is composed of just one input variable "x" and one output
                variable "y"
              </p>
              <p>
                x = {1, 3, 2, 4, 6, 5} <br />
                y = {1, 2, 3, 3, 2, 5}
              </p>
              <p>
                A dot graph of the data can be seen in Figure 1.
                The model will be a straight line defined by the equation:
              </p>
              <p>
                y = b0 + b1 * x
              </p>
              <p>
                Where b0 and b1 are the coefficients we need to estimate.
                The first method of calculating these coefficients will be
                by using the following formulas:
              </p>
              <p>
                b1 = &sum; (x<sub>i</sub> - mean(x)) * (y<sub>i</sub> - mean(y)) / &sum; (x<sub>i</sub> - mean(x))<sup>2</sup><br />
                b0 = mean(y) - b1 * mean(x)
              </p>
              <p>
                This gives us the following coefficients:
              </p>
              <p>
                b1 = 0.342857143<br />
                b0 = 1.466666667
              </p>
              <p>
                By using these in the previous equation, we can now
                plot the line that forms the model. This graph is shown in Figure 2.
                Finally, we can calculate the RMSE to get an estimate of the accuracy
                of this model:
              </p>
              <p>
                RMSE = 1.101225868
              </p>
              <p>
                The second method of calculating the coefficients involves applying
                gradient descent. Starting from b0 = 0.0 and b1 = 0.0, we can recalculate
                them at every epoch by performing:
              </p>
              <p>
                b0(next epoch) = b0 - learn rate * prediction error<br />
                b1(next epoch) = b1 - learn rate * prediction error
              </p>
              <p>
                Using a learn rate of 0.01, the final values of the
                coefficients after 24 epochs is: 
              </p>
              <p>
                b1 = 0.638197357<br />
                b0 = 0.217811901
              </p>
              <p>
                This gives us another model for our data. The resulting line graph
                can be seen in Figure 3. As we can see, this model is slightly different from
                the previous one. The RMSE for this model is:
              </p>
              <p>
                RMSE = 1.230204322
              </p>
              <p>
                The higher error indicates that the first model was a bit more accurate,
                even though both lines like a good approximation to the original dots.
                We can presume that this is possibly due to the limited number of epochs
                in the gradient descent. Ideally, we would want to continue the process
                until there is no significant reduction in the prediction errors.
              </p>
              <!-- <table>
                <tr>
                  <th>x</th>
                  <th>predicted y</th>
                </tr>
                <tr>
                  <td>1</td>
                  <td>1.80952381</td>
                </tr>
                <tr>
                  <td>3</td>
                  <td>2.495238095</td>
                </tr>
                <tr>
                  <td>2</td>
                  <td>2.152380952</td>
                </tr>
                <tr>
                  <td>4</td>
                  <td>2.838095238</td>
                </tr>
                <tr>
                  <td>6</td>
                  <td>3.523809524</td>
                </tr>
                <tr>
                  <td>5</td>
                  <td>3.180952381</td>
                </tr>
              </table> -->
            </div>
            <div class="col-6 col-12-medium">
              <span class="image fit">
                <img src="./images/tasks/graph1.PNG" alt=""/>
                <figcaption>Fig.1</figcaption>
              </span>
              <span class="image fit">
                <img src="./images/tasks/graph2.PNG" alt=""/>
                <figcaption>Fig.2</figcaption>
              </span>
              <span class="image fit">
                <img src="./images/tasks/graph3.PNG" alt=""/>
                <figcaption>Fig.3</figcaption>
              </span>
            </div>
          </div>
          <hr />
          <h2 class="major"><a id="titanic">Dataset "Titanic"</a></h2>
          <p>
            In this exercise we will be processing messy data from the popular 
            "Titanic" dataset (taken from Kaggle), so that it is ready to use for 
            bulding a model that can predict whether a given passenger survived or 
            not. However, this is focused solely on preprocessing of data and not 
            modelling.
          </p>
          <p>
            The exercise was done in Python, using libraries such as Numpy, Pandas,
            sklearn, and more. The entire process is documented in
            <a href="https://www.kaggle.com/franciscopiria/ut2-pd4/edit/run/46907318">this Kaggle notebook.</a>
          </p>
          <hr />
          <h2 class="major"><a id="cardiac">Prediction of second cardiac arrest</a></h1>
          <div class="row">
            <div class="col-6 col-12-medium">
              <p>
                This exercise is based on the case of a cardiologist who
                wishes to offer special treatments to patients of his who 
                have an increased risk of suffering a second heart attack, in order
                to reduce the risk.
                The doctor has access to the medical records of all his patients,
                and wants to use these records to determine those who would benefit
                the most from these treatments. To this end, we will generate
                a logistic regression model with the help of RapidMiner. 
              </p>
              <p>
                The database of medical records contains two datasets. The first one
                contains the records of patients who have already suffered a heart attack, 
                with an attribute to indicate whether they had a second one or not. 
                This dataset wil be used to train the model. The second dataset contains
                records of patients who have suffered one, but not yet two heart attacks.
                This is the data that will be used for prediction.
              </p>
              <p>
                When we import the data into RapidMiner, first we need to convert the label 
                attribute ("2do_Ataque_Corazon") from polynominal into binominal, since we want
                to predict between two classes (yes/no). Then we also want to make sure that every other 
                attribute in the dataset has only numerical values, which they do. 
                This is necessary since it's the only type of value accepted by the 
                logistic regression operator. Finally, we also need to check that the ranges 
                of the values seen in the training and testing datasets are similar. 
                This is important because a large amount of data is required to train a 
                model, and there may not be enough useful data locally. It may be 
                necessary to use data from other places in the world for training, 
                therefore it is necessary to verify that the examples are similar to
                those for which the model will attempt to predict. If not, the model 
                would be trained with data that does not match the local data, 
                and it would give incorrect results.<br />
                The statistics of the attributes can be seen in Figure 1 for the training
                dataset, and in Figure 2 for the prediction dataset. We can see here that the ranges
                are indeed the same.
              </p>
              <p>
                With all that done, we can now finish the RapidMiner process, which can 
                be seen in Figure 3. Out of the 690 patients in the testing
                dataset, the model predicted that 350 will suffer a second
                heart attack more likely than not. As an example, the confidence
                of the first 20 predictions is shown in Figure 4.
              </p>
              <p>
                The doctor would like to treat as many patients as he can, 
                even if their risk is low. But still, resources are limited.
                Given that only a limited number of patients can receive the 
                treatment, in a real system it would be necessary to define a 
                threshold for the confidence 
                of the model's prediction. Only patients whose prediction of having a 
                second attack is above the confidence threshold would be treated,
                since those would be the ones most at risk.
              </p>
            </div>
            <div class="col-6 col-12-medium">
                <span class="image fit">
                  <img src="./images/tasks/stat1.PNG" alt=""/>
                  <figcaption>Fig.1</figcaption>
                </span>
                <span class="image fit">
                  <img src="./images/tasks/stat2.PNG" alt=""/>
                  <figcaption>Fig.2</figcaption>
                </span>
                <span class="image fit">
                  <img src="./images/tasks/process1.PNG" alt=""/>
                  <figcaption>Fig.3</figcaption>
                </span>
                <span class="image fit">
                  <img src="./images/tasks/prediction1.PNG" alt=""/>
                  <figcaption>Fig.4</figcaption>
                </span>
            </div>
          </div>
          <hr />
          <h2 class="major"><a id="cereal">Cereal nutrition data analysis</a></h1>
          <div class="row">
            <div class="col-6 col-12-medium">
              <p>
                In this exercise we will be working with a dataset containing
                information about 80 different cereal products. Most of the
                fields in the dataset are related to the nutrients found in
                these cereals, such as calories, protein, sodium, and more.
                The dataset contains 16 attributes, 13 of which are numerical.
                The objective of this exercise is to reduce these 13 attributes
                into a smaller subset, applying Principal Component Analysis
                in RapidMiner.
              </p>
              <p>
                The first thing we need to do when importing the data is to
                filter the attributes "name", "manufacturer", and "type" since 
                they are non-numerical, meaning that they can't be used for PCA.<br />
                PCA also requires that there be no missing values, so 
                we need to replace them. Luckily, there are only 4 missing values
                in the entire dataset, so we can replace them with averages without
                it having a significant impact.<br />
                The last thing to do before applying PCA is normalizing the data.
                If there was one atribute for which the range was hundreds of times larger
                than the ranges of other ones, then that attribute would form 
                most of the variance in the dataset, and PCA would wrongly identify
                it as the most significant one. PCA is very sensitive to scaling
                in the data, so it's necessary to apply a range transformation
                to make sure that it doesn't overlook any attributte.<br /> 
                For the PCA operator we select "keep variance" as the type
                of dimensionality reduction, since we want a subset of attributes
                that account for a certain amount of variance in the dataset,
                but we don't know beforehand how many principal components it would
                take. For the variance threshold we use the default 0.95. The process
                is shown in Figure 1. 
              </p>
              <p>
                When we run the process, we can see the resulting data in Figure 2, 
                along with the eigenvalues in Figure 3. The cumulative variance of
                the first 8 principal components was enough to account for 97%
                of the dataset's total variance. In Figure 4 we can see the eigenvectors,
                which show us how the principal components are composed. We want 
                to identify the original attributes which had the most 
                weight in these PCs. To do that we can sort the eigenvalues
                for each PC, and then select the 2 or 3 most impactful attributes.
                If we select the 2 most significant attributes for each of 
                the first 8 PCs, we end up with a subset of 8 attributes:
                "shelf", "potass", "sugars", "calories", "sodium", "carbo", 
                "vitamins" and "protein".<br />
                For this example exercise, we were able to reduce a set of
                13 attributes down to just 8 (more than a third of the original
                amount). It may not mean much with such a small number of attributes,
                but with very large datasets that contain hundreds or thousands of 
                attributes it would make a huge difference in terms of the 
                computational performance of various machine learning algorithms.
                This is why PCA is such a widely used tool for dimensionality reduction.
              </p>
            </div>
            <div class="col-6 col-12-medium">
                <span class="image fit">
                  <img src="./images/tasks/pca1.PNG" alt=""/>
                  <figcaption>Fig.1</figcaption>
                </span>
                <span class="image fit">
                  <img src="./images/tasks/pca2.PNG" alt=""/>
                  <figcaption>Fig.2</figcaption>
                </span>
                <span class="image fit">
                  <img src="./images/tasks/pca3.PNG" alt=""/>
                  <figcaption>Fig.3</figcaption>
                </span>
                <span class="image fit">
                  <img src="./images/tasks/pca4.PNG" alt=""/>
                  <figcaption>Fig.4</figcaption>
                </span>
            </div>
          </div>
        </div>
      </section>
    </div>

    <!-- Footer -->
    <footer id="footer" class="wrapper alt">
      <div class="inner"></div>
    </footer>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
  </body>
</html>
